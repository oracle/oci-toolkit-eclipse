package com.oracle.oci.eclipse.ui.explorer.dataflow;

import java.util.HashMap;
import java.util.Map;

public class DataflowConstants {

	public static final int numOfExecutorsMin = 1;
	public static final int numOfExecutorsMax = 10;
	public static final int numOfExecutorsDefault = 1;
	public static final int numOfExecutorsIncrement = 1;
	public static final String[] versions = new String[] { "2.4.4","3.0.2" };			 
			
	public static String[] shapesDetails = new String[] {
			"VM.Standard2.1 (15 GB Memory, 1 OCPU, 175 GB Block Volume)" ,
			"VM.Standard2.2 (30 GB Memory, 2 OCPU, 275 GB Block Volume)" ,
			"VM.Standard2.4 (60 GB Memory, 4 OCPU, 475 GB Block Volume)" ,
			"VM.Standard2.8 (120 GB Memory, 8 OCPU, 875 GB Block Volume)" ,
			"VM.Standard2.16 (240 GB Memory, 16 OCPU, 1675 GB Block Volume)"
	};
	
	public static final String[] spark2PropertiesList =  {
       "spark.driver.maxResultSize",
       "spark.logConf",
       "spark.driver.extraJavaOptions",
       "spark.driver.userClassPathFirst",
       "spark.driverEnv.*",
       "spark.executor.extraJavaOptions",
       "spark.executorEnv.*",
       "spark.redaction.regex",
       "spark.python.profile",
       "spark.reducer.maxSizeInFlight",
       "spark.reducer.maxReqsInFlight",
       "spark.reducer.maxBlocksInFlightPerAddress",
       "spark.maxRemoteBlockSizeFetchToMem",
       "spark.shuffle.compress",
       "spark.shuffle.file.buffer",
       "spark.shuffle.io.maxRetries",
       "spark.shuffle.io.numConnectionsPerPeer",
       "spark.shuffle.io.preferDirectBufs",
       "spark.shuffle.io.retryWait",
       "spark.shuffle.io.backLog",
       "spark.shuffle.service.enabled",
       "spark.shuffle.service.port",
       "spark.shuffle.service.index.cache.size",
       "spark.shuffle.maxChunksBeingTransferred",
       "spark.shuffle.sort.bypassMergeThreshold",
       "spark.shuffle.spill.compress",
       "spark.shuffle.accurateBlockThreshold",
       "spark.shuffle.registration.timeout",
       "spark.shuffle.registration.maxAttempts",
       "spark.eventLog.enabled",
       "spark.eventLog.logBlockUpdates.enabled",
       "spark.eventLog.longForm.enabled",
       "spark.ui.dagGraph.retainedRootRDDs",
       "spark.ui.killEnabled",
       "spark.ui.liveUpdate.minFlushPeriod",
       "spark.ui.retainedJobs",
       "spark.ui.retainedStages",
       "spark.ui.retainedTasks",
       "spark.ui.showConsoleProgress",
       "spark.worker.ui.retainedExecutors",
       "spark.worker.ui.retainedDrivers",
       "spark.sql.ui.retainedExecutions",
       "spark.sql.inMemoryColumnarStorage.compressed",
       "spark.sql.inMemoryColumnarStorage.batchSize",
       "spark.sql.files.maxPartitionBytes",
       "spark.sql.files.openCostInBytes",
       "spark.sql.broadcastTimeout",
       "spark.sql.autoBroadcastJoinThreshold",
       "spark.sql.orc.impl",
       "spark.sql.shuffle.partitions",
       "spark.sql.crossJoin.enabled",
       "spark.ui.retainedDeadExecutors",
       "spark.ui.filters",
       "spark.ui.requestHeaderSize",
       "spark.broadcast.compress",
       "spark.checkpoint.compress",
       "spark.io.compression.codec",
       "spark.io.compression.lz4.blockSize",
       "spark.io.compression.snappy.blockSize",
       "spark.io.compression.zstd.level",
       "spark.io.compression.zstd.bufferSize",
       "spark.kryo.classesToRegister",
       "spark.kryo.referenceTracking",
       "spark.kryo.registrationRequired",
       "spark.kryo.registrator",
       "spark.kryo.unsafe",
       "spark.kryoserializer.buffer.max",
       "spark.kryoserializer.buffer",
       "spark.rdd.compress",
       "spark.serializer",
       "spark.serializer.objectStreamReset",
       "spark.memory.fraction",
       "spark.memory.storageFraction",
       "spark.memory.offHeap.enabled",
       "spark.memory.offHeap.size",
       "spark.memory.useLegacyMode",
       "spark.storage.replication.proactive",
       "spark.cleaner.periodicGC.interval",
       "spark.cleaner.referenceTracking",
       "spark.cleaner.referenceTracking.blocking",
       "spark.cleaner.referenceTracking.blocking.shuffle",
       "spark.cleaner.referenceTracking.cleanCheckpoints",
       "spark.broadcast.blockSize",
       "spark.broadcast.checksum",
       "spark.default.parallelism",
       "spark.executor.heartbeatInterval",
       "spark.jars",
       "spark.files",
       "spark.files.fetchTimeout",
       "spark.files.useFetchCache",
       "spark.files.overwrite",
       "spark.hadoop.cloneConf",
       "spark.storage.memoryMapThreshold",
       "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version",
       "spark.rpc.message.maxSize",
       "spark.blockManager.port",
       "spark.driver.blockManager.port",
       "spark.rpc.io.backLog",
       "spark.network.timeout",
       "spark.port.maxRetries",
       "spark.rpc.numRetries",
       "spark.rpc.retry.wait",
       "spark.rpc.askTimeout",
       "spark.rpc.lookupTimeout",
       "spark.core.connection.ack.wait.timeout",
       "spark.scheduler.maxRegisteredResourcesWaitingTime",
       "spark.scheduler.minRegisteredResourcesRatio",
       "spark.scheduler.mode",
       "spark.scheduler.revive.interval",
       "spark.scheduler.listenerbus.eventqueue.capacity",
       "spark.scheduler.blacklist.unschedulableTaskSetTimeout",
       "spark.blacklist.enabled",
       "spark.blacklist.timeout",
       "spark.blacklist.task.maxTaskAttemptsPerExecutor",
       "spark.blacklist.task.maxTaskAttemptsPerNode",
       "spark.blacklist.stage.maxFailedTasksPerExecutor",
       "spark.blacklist.stage.maxFailedExecutorsPerNode",
       "spark.blacklist.application.maxFailedTasksPerExecutor",
       "spark.blacklist.application.maxFailedExecutorsPerNode",
       "spark.blacklist.killBlacklistedExecutors",
       "spark.blacklist.application.fetchFailure.enabled",
       "spark.speculation",
       "spark.speculation.interval",
       "spark.speculation.multiplier",
       "spark.speculation.quantile",
       "spark.submit.pyFiles",
       "spark.task.maxFailures",
       "spark.task.reaper.enabled",
       "spark.task.reaper.pollingInterval",
       "spark.task.reaper.threadDump",
       "spark.task.reaper.killTimeout",
       "spark.task.cpus",
       "spark.stage.maxConsecutiveAttempts"
	};
	
	public static final String[] spark3PropertiesList = {
            "spark.driver.maxResultSize",
            "spark.logConf",
            "spark.driver.extraJavaOptions",
            "spark.driver.userClassPathFirst",
            "spark.driverEnv.*",
            "spark.executor.extraJavaOptions",
            "spark.executorEnv.*",
            "spark.redaction.regex",
            "spark.python.profile",
            "spark.reducer.maxSizeInFlight",
            "spark.reducer.maxReqsInFlight",
            "spark.reducer.maxBlocksInFlightPerAddress",
            "spark.maxRemoteBlockSizeFetchToMem",
            "spark.shuffle.compress",
            "spark.shuffle.file.buffer",
            "spark.shuffle.io.maxRetries",
            "spark.shuffle.io.numConnectionsPerPeer",
            "spark.shuffle.io.preferDirectBufs",
            "spark.shuffle.io.retryWait",
            "spark.shuffle.io.backLog",
            "spark.shuffle.service.enabled",
            "spark.shuffle.service.port",
            "spark.shuffle.service.index.cache.size",
            "spark.shuffle.maxChunksBeingTransferred",
            "spark.shuffle.sort.bypassMergeThreshold",
            "spark.shuffle.spill.compress",
            "spark.shuffle.accurateBlockThreshold",
            "spark.shuffle.registration.timeout",
            "spark.shuffle.registration.maxAttempts",
            "spark.eventLog.enabled",
            "spark.eventLog.logBlockUpdates.enabled",
            "spark.eventLog.longForm.enabled",
            "spark.ui.dagGraph.retainedRootRDDs",
            "spark.ui.killEnabled",
            "spark.ui.liveUpdate.minFlushPeriod",
            "spark.ui.retainedJobs",
            "spark.ui.retainedStages",
            "spark.ui.retainedTasks",
            "spark.ui.showConsoleProgress",
            "spark.worker.ui.retainedExecutors",
            "spark.worker.ui.retainedDrivers",
            "spark.sql.ui.retainedExecutions",
            "spark.sql.inMemoryColumnarStorage.compressed",
            "spark.sql.inMemoryColumnarStorage.batchSize",
            "spark.sql.files.maxPartitionBytes",
            "spark.sql.files.openCostInBytes",
            "spark.sql.broadcastTimeout",
            "spark.sql.autoBroadcastJoinThreshold",
            "spark.sql.orc.impl",
            "spark.sql.shuffle.partitions",
            "spark.sql.crossJoin.enabled",
            "spark.ui.retainedDeadExecutors",
            "spark.ui.filters",
            "spark.ui.requestHeaderSize",
            "spark.broadcast.compress",
            "spark.checkpoint.compress",
            "spark.io.compression.codec",
            "spark.io.compression.lz4.blockSize",
            "spark.io.compression.snappy.blockSize",
            "spark.io.compression.zstd.level",
            "spark.io.compression.zstd.bufferSize",
            "spark.kryo.classesToRegister",
            "spark.kryo.referenceTracking",
            "spark.kryo.registrationRequired",
            "spark.kryo.registrator",
            "spark.kryo.unsafe",
            "spark.kryoserializer.buffer.max",
            "spark.kryoserializer.buffer",
            "spark.rdd.compress",
            "spark.serializer",
            "spark.serializer.objectStreamReset",
            "spark.memory.fraction",
            "spark.memory.storageFraction",
            "spark.memory.offHeap.enabled",
            "spark.memory.offHeap.size",
            "spark.memory.useLegacyMode",
            "spark.storage.replication.proactive",
            "spark.cleaner.periodicGC.interval",
            "spark.cleaner.referenceTracking",
            "spark.cleaner.referenceTracking.blocking",
            "spark.cleaner.referenceTracking.blocking.shuffle",
            "spark.cleaner.referenceTracking.cleanCheckpoints",
            "spark.broadcast.blockSize",
            "spark.broadcast.checksum",
            "spark.default.parallelism",
            "spark.executor.heartbeatInterval",
            "spark.jars",
            "spark.files",
            "spark.files.fetchTimeout",
            "spark.files.useFetchCache",
            "spark.files.overwrite",
            "spark.hadoop.cloneConf",
            "spark.storage.memoryMapThreshold",
            "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version",
            "spark.rpc.message.maxSize",
            "spark.blockManager.port",
            "spark.driver.blockManager.port",
            "spark.rpc.io.backLog",
            "spark.network.timeout",
            "spark.port.maxRetries",
            "spark.rpc.numRetries",
            "spark.rpc.retry.wait",
            "spark.rpc.askTimeout",
            "spark.rpc.lookupTimeout",
            "spark.core.connection.ack.wait.timeout",
            "spark.scheduler.maxRegisteredResourcesWaitingTime",
            "spark.scheduler.minRegisteredResourcesRatio",
            "spark.scheduler.mode",
            "spark.scheduler.revive.interval",
            "spark.scheduler.listenerbus.eventqueue.capacity",
            "spark.scheduler.blacklist.unschedulableTaskSetTimeout",
            "spark.blacklist.enabled",
            "spark.blacklist.timeout",
            "spark.blacklist.task.maxTaskAttemptsPerExecutor",
            "spark.blacklist.task.maxTaskAttemptsPerNode",
            "spark.blacklist.stage.maxFailedTasksPerExecutor",
            "spark.blacklist.stage.maxFailedExecutorsPerNode",
            "spark.blacklist.application.maxFailedTasksPerExecutor",
            "spark.blacklist.application.maxFailedExecutorsPerNode",
            "spark.blacklist.killBlacklistedExecutors",
            "spark.blacklist.application.fetchFailure.enabled",
            "spark.speculation",
            "spark.speculation.interval",
            "spark.speculation.multiplier",
            "spark.speculation.quantile",
            "spark.submit.pyFiles",
            "spark.task.maxFailures",
            "spark.task.reaper.enabled",
            "spark.task.reaper.pollingInterval",
            "spark.task.reaper.threadDump",
            "spark.task.reaper.killTimeout",
            "spark.task.cpus",
            "spark.stage.maxConsecutiveAttempts",
            "spark.sql.adaptive.*",
            "spark.kerberos.keytab",
            "spark.kerberos.principal"};
	

	
}
